{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq6wFb3+hDmQoqp1e2lRAi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmiJoba22/RateMD-Datapipe/blob/main/pythonscripts%20/%20extract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uium1FgAF-nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ac993b-e186-4c3a-957c-3b6b6fa318f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-storage-blob\n",
            "  Downloading azure_storage_blob-12.25.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting azure-core>=1.30.0 (from azure-storage-blob)\n",
            "  Downloading azure_core-1.34.0-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob) (43.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob) (4.14.0)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2025.6.15)\n",
            "Downloading azure_storage_blob-12.25.1-py3-none-any.whl (406 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m407.0/407.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.34.0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.4/207.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, azure-core, azure-storage-blob\n",
            "Successfully installed azure-core-1.34.0 azure-storage-blob-12.25.1 isodate-0.7.2\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.13.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading pymongo-4.13.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.7.0 pymongo-4.13.2\n"
          ]
        }
      ],
      "source": [
        "#Get data from source to put in destination\n",
        "#Destination has an address; source has an address\n",
        "!pip install azure-storage-blob\n",
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient"
      ],
      "metadata": {
        "id": "FiReFCHWIHct"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the config file that is json\n",
        "import json\n",
        "with open('config.json') as config_file:\n",
        "  config = json.load(config_file)\n",
        "\n",
        "#read information about the destination\n",
        "DESTINATION_CONNECTION_STRING = config['DESTINATION_CONNECTION_STRING']\n",
        "DESTINATION_CONTAINER_NAME = config['DESTINATION_CONTAINER_NAME']\n",
        "DESTINATION_FILE_NAME = config ['DESTINATION_FILE_NAME']\n",
        "\n",
        "# read information about mongodb\n",
        "MONGODB_CONNECTION_STRING = config ['MONGODB_CONNECTION_STRING']\n",
        "MONGO_DB_NAME = config ['MONGO_DB_NAME']\n",
        "MONGO_COLLECTION_NAME = config ['MONGO_COLLECTION_NAME']\n",
        "print(MONGO_COLLECTION_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqGqq8z5IhRe",
        "outputId": "52e452cd-7839-44d7-f4d9-fd37229c71dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ratemd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to mongodb and test the connection using ping\n",
        "client = MongoClient(MONGODB_CONNECTION_STRING)\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "db = client[MONGO_DB_NAME]\n",
        "collection = db[MONGO_COLLECTION_NAME]\n",
        "print(collection)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyGhWu7cJGag",
        "outputId": "c70483f6-35e8-4469-bbae-df73fee3d8f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinged your deployment. You successfully connected to MongoDB!\n",
            "Collection(Database(MongoClient(host=['20.232.135.212:27017'], document_class=dict, tz_aware=False, connect=True, authmechanism='DEFAULT', authsource='admin'), 'healthrate'), 'ratemd')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.storage.blob import BlobServiceClient\n",
        "import pandas as pd\n",
        "\n",
        "CHUNK_SIZE = 100000\n",
        "total_docs = collection.count_documents({})\n",
        "print(f\"Total documents in collection: {total_docs}\")\n",
        "\n",
        "blob_service_client = BlobServiceClient.from_connection_string(DESTINATION_CONNECTION_STRING)\n",
        "container_client = blob_service_client.get_container_client(DESTINATION_CONTAINER_NAME)\n",
        "\n",
        "# Create the container if it does not exist\n",
        "try:\n",
        "    container_client.create_container()\n",
        "    print(f\"Container '{DESTINATION_CONTAINER_NAME}' created.\")\n",
        "except Exception as e:\n",
        "    # Handle exceptions, e.g., if the container already exists\n",
        "    print(f\"Could not create container '{DESTINATION_CONTAINER_NAME}': {e}\")\n",
        "\n",
        "\n",
        "for skip in range(0, total_docs, CHUNK_SIZE):\n",
        "    chunk_count = (skip // CHUNK_SIZE) + 1\n",
        "    blob_chunk_name = f\"{DESTINATION_FILE_NAME}_chunk_{chunk_count}.csv\"\n",
        "\n",
        "    # ğŸ›‘ Skip if blob already exists\n",
        "    if container_client.get_blob_client(blob_chunk_name).exists():\n",
        "        print(f\"â­ï¸ Chunk {chunk_count} already exists in Azure. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    print(f\"ğŸš§ Processing chunk {chunk_count}\")\n",
        "\n",
        "    # Fetch a chunk of documents\n",
        "    cursor = collection.find().skip(skip).limit(CHUNK_SIZE)\n",
        "    chunk_data = list(cursor)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df_chunk = pd.json_normalize(chunk_data)\n",
        "\n",
        "    # Save to CSV\n",
        "    local_chunk_file_name = f\"mongo_collection_data_chunk_{chunk_count}.csv\"\n",
        "    df_chunk.to_csv(local_chunk_file_name, index=False)\n",
        "\n",
        "    # Upload to Azure\n",
        "    with open(local_chunk_file_name, \"rb\") as data:\n",
        "        container_client.upload_blob(name=blob_chunk_name, data=data, overwrite=True)\n",
        "\n",
        "    print(f\"âœ… Chunk {chunk_count} uploaded as '{blob_chunk_name}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQnBEWWLXsM9",
        "outputId": "3e521f84-9cbd-4277-9482-0b8a8eedf82d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents in collection: 2204490\n",
            "Could not create container 'ratemd': The specified container already exists.\n",
            "RequestId:34afabab-f01e-006a-5ea8-eb5db4000000\n",
            "Time:2025-07-02T23:25:13.6840762Z\n",
            "ErrorCode:ContainerAlreadyExists\n",
            "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>ContainerAlreadyExists</Code><Message>The specified container already exists.\n",
            "RequestId:34afabab-f01e-006a-5ea8-eb5db4000000\n",
            "Time:2025-07-02T23:25:13.6840762Z</Message></Error>\n",
            "ğŸš§ Processing chunk 1\n",
            "âœ… Chunk 1 uploaded as '_chunk_1.csv'\n",
            "ğŸš§ Processing chunk 2\n",
            "âœ… Chunk 2 uploaded as '_chunk_2.csv'\n",
            "ğŸš§ Processing chunk 3\n",
            "âœ… Chunk 3 uploaded as '_chunk_3.csv'\n",
            "ğŸš§ Processing chunk 4\n",
            "âœ… Chunk 4 uploaded as '_chunk_4.csv'\n",
            "ğŸš§ Processing chunk 5\n",
            "âœ… Chunk 5 uploaded as '_chunk_5.csv'\n",
            "ğŸš§ Processing chunk 6\n",
            "âœ… Chunk 6 uploaded as '_chunk_6.csv'\n",
            "ğŸš§ Processing chunk 7\n",
            "âœ… Chunk 7 uploaded as '_chunk_7.csv'\n",
            "ğŸš§ Processing chunk 8\n",
            "âœ… Chunk 8 uploaded as '_chunk_8.csv'\n",
            "ğŸš§ Processing chunk 9\n",
            "âœ… Chunk 9 uploaded as '_chunk_9.csv'\n",
            "ğŸš§ Processing chunk 10\n",
            "âœ… Chunk 10 uploaded as '_chunk_10.csv'\n",
            "ğŸš§ Processing chunk 11\n",
            "âœ… Chunk 11 uploaded as '_chunk_11.csv'\n",
            "ğŸš§ Processing chunk 12\n",
            "âœ… Chunk 12 uploaded as '_chunk_12.csv'\n",
            "ğŸš§ Processing chunk 13\n",
            "âœ… Chunk 13 uploaded as '_chunk_13.csv'\n",
            "ğŸš§ Processing chunk 14\n",
            "âœ… Chunk 14 uploaded as '_chunk_14.csv'\n",
            "ğŸš§ Processing chunk 15\n",
            "âœ… Chunk 15 uploaded as '_chunk_15.csv'\n",
            "ğŸš§ Processing chunk 16\n",
            "âœ… Chunk 16 uploaded as '_chunk_16.csv'\n",
            "ğŸš§ Processing chunk 17\n",
            "âœ… Chunk 17 uploaded as '_chunk_17.csv'\n",
            "ğŸš§ Processing chunk 18\n",
            "âœ… Chunk 18 uploaded as '_chunk_18.csv'\n",
            "ğŸš§ Processing chunk 19\n",
            "âœ… Chunk 19 uploaded as '_chunk_19.csv'\n",
            "ğŸš§ Processing chunk 20\n",
            "âœ… Chunk 20 uploaded as '_chunk_20.csv'\n",
            "ğŸš§ Processing chunk 21\n",
            "âœ… Chunk 21 uploaded as '_chunk_21.csv'\n",
            "ğŸš§ Processing chunk 22\n",
            "âœ… Chunk 22 uploaded as '_chunk_22.csv'\n",
            "ğŸš§ Processing chunk 23\n",
            "âœ… Chunk 23 uploaded as '_chunk_23.csv'\n"
          ]
        }
      ]
    }
  ]
}